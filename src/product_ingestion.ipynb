{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c650d404",
   "metadata": {},
   "source": [
    "##### Connect to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "90b13c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from fuzzywuzzy import process\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "base_dir = \"/Users/sairamlingineni/Documents/lilohq/\"\n",
    "\n",
    "load_dotenv(os.path.join(base_dir, \".env\"))\n",
    "\n",
    "ES_HOST = os.getenv(\"ES_HOST\")\n",
    "ES_API_KEY = os.getenv(\"ES_API_KEY\")\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=[ES_HOST],\n",
    "    api_key=ES_API_KEY,\n",
    ")\n",
    "\n",
    "# Check connection\n",
    "if es.ping():\n",
    "    print(\"Connected to Elasticsearch\")\n",
    "else:\n",
    "    print(\"Connection failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c453b3c",
   "metadata": {},
   "source": [
    "##### CREATE SYNOMYS USING THE ES `_synonyms` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "366d3b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 'updated', 'reload_analyzers_details': {'_shards': {'total': 6, 'successful': 6, 'failed': 0}, 'reload_details': [{'index': 'products', 'reloaded_analyzers': ['search_analyzer'], 'reloaded_node_ids': ['vCLd-XP4TUajog5Gn6sPOg', 'ZBjHBxzCTWmEuqPwXWmo9Q']}]}}\n"
     ]
    }
   ],
   "source": [
    "syn_path = f\"{base_dir}data/synonyms.json\"\n",
    "\n",
    "with open(syn_path, \"r\") as f:\n",
    "    syn_json = json.load(f)\n",
    "\n",
    "syn_set = [{\"synonyms\": f\"{a}, {b}\"} for a, b in syn_json]\n",
    "\n",
    "resp = es.synonyms.put_synonym(\n",
    "    id=\"product-synonyms\",\n",
    "    synonyms_set=syn_set\n",
    ")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb3b01",
   "metadata": {},
   "source": [
    "##### Create the inference ( Here using Elastic `Elser` model and default `semantic_text` fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d7158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'\\nresp = es.inference.put(\\n    task_type=\"sparse_embedding\",\\n    inference_id=\"product-elser-model\",\\n    inference_config={\\n        \"service\": \"elser\",\\n        \"service_settings\": {\\n            \"num_allocations\": 1,\\n            \"num_threads\": 4\\n        }\\n    },\\n)\\nprint(resp)\\n'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "resp = es.inference.put(\n",
    "    task_type=\"sparse_embedding\",\n",
    "    inference_id=\"product-elser-model\",\n",
    "    inference_config={\n",
    "        \"service\": \"elser\",\n",
    "        \"service_settings\": {\n",
    "            \"num_allocations\": 1,\n",
    "            \"num_threads\": 4\n",
    "        }\n",
    "    },\n",
    ")\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f925847",
   "metadata": {},
   "source": [
    "##### Create a pipeline for Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e4daef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "pipeline_file = Path(base_dir) / \"data\" / \"products_pipeline.json\"\n",
    "with open(pipeline_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    processors = json.load(f)\n",
    "resp = es.ingest.put_pipeline(\n",
    "    id=\"product_pipeline\",\n",
    "    processors=processors\n",
    ")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb1f07",
   "metadata": {},
   "source": [
    "##### Create `products` and `products_errors` template #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9fa61198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "template_file = Path(base_dir) / \"data\" / \"products_template.json\"\n",
    "with open(template_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    template_json = json.load(f)\n",
    "resp = es.indices.put_index_template(\n",
    "    name=\"products_template\",\n",
    "    index_patterns=[\"products\"],\n",
    "    template=template_json,\n",
    "    priority=1000\n",
    "    )\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "255ca780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "template_file = Path(base_dir) / \"data\" / \"products_errors_template.json\"\n",
    "with open(template_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    template_json = json.load(f)\n",
    "resp = es.indices.put_index_template(\n",
    "    name=\"products_errors_template\",\n",
    "    index_patterns=[\"products_errors\"],\n",
    "    template=template_json,\n",
    "    priority=1000\n",
    "    )\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4a79f",
   "metadata": {},
   "source": [
    "##### CREATE `PRODUCT` and `PRODUCT_ERRORS` INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587cb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nesp = es.indices.create(\\n    index=\"products\",\\n)\\nprint(resp)\\n'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "esp = es.indices.create(\n",
    "    index=\"products\",\n",
    ")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c66d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nesp = es.indices.create(\\n    index=\"products_errors\",\\n)\\nprint(resp)\\n\\n'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "esp = es.indices.create(\n",
    "    index=\"products_errors\",\n",
    ")\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c87d9",
   "metadata": {},
   "source": [
    "##### Read the Products from the `data/products.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d49fa1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_file = Path(base_dir) / \"data\" / \"products.json\"\n",
    "with open(products_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    products = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f418e424",
   "metadata": {},
   "source": [
    "##### Create a Normalizer function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cbff5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_attributes(raw_attributes, canonical_keys, threshold=60): # adjusted threshold\n",
    "    \"\"\"\n",
    "    Normalize attribute keys using fuzzy matching against canonical keys.\n",
    "    Keeps unknown keys in lowercase if no close match is found.\n",
    "    \"\"\"\n",
    "    normalized = {}\n",
    "    new_attributes = []\n",
    "    for k, v in raw_attributes.items():\n",
    "        best_match, score = process.extractOne(k.lower(), canonical_keys)\n",
    "        if score >= threshold:\n",
    "            normalized[best_match] = v\n",
    "        else:\n",
    "            normalized[k.lower()] = v \n",
    "            new_attributes.append(k)\n",
    "    return normalized, new_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87687a0c",
   "metadata": {},
   "source": [
    "##### Fix the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8248875c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bulk_pack', 'power_in_hp', 'color', 'material', 'diameter_mm', 'pack_size', 'notes', 'flow_rate_lpm', 'hp', 'voltage']\n"
     ]
    }
   ],
   "source": [
    "attributes_file = Path(base_dir) / \"data\" / \"attributes.json\"\n",
    "with open(attributes_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    canonical_att = json.load(f)\n",
    "print(canonical_att)\n",
    "\n",
    "for product in products:\n",
    "    if \"attributes\" in product:\n",
    "        normalized_attrs, new_attr = normalize_attributes(\n",
    "            product[\"attributes\"], canonical_att\n",
    "        )\n",
    "\n",
    "        product[\"attributes\"] = normalized_attrs \n",
    "\n",
    "        if new_attr:\n",
    "            product[\"new_attributes\"] = list(set(new_attr))\n",
    "            print(product[\"attributes\"] , product[\"new_attributes\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f1a0d",
   "metadata": {},
   "source": [
    "##### Normalization for Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a3e13fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_category(raw_category, canonical_categories, threshold=60):\n",
    "    \"\"\"\n",
    "    Normalize hierarchical product categories.\n",
    "    \n",
    "    Fixes:\n",
    "      - spelling errors\n",
    "      - level-by-level mismatch\n",
    "      - wrong order\n",
    "      - spacing\n",
    "    Returns:\n",
    "      normalized_category (str)\n",
    "      is_new (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    raw_parts = [p.strip().lower() for p in raw_category.split(\">\")]\n",
    "    #print(\"Raw parts:\", raw_parts)\n",
    "\n",
    "    # Split canonical categories into 2D lists\n",
    "    canonical_split = [c.split(\" > \") for c in canonical_categories]\n",
    "    flat_no_duplicates = list(dict.fromkeys(item for sublist in canonical_split for item in sublist))\n",
    "\n",
    "    reconstructed = []\n",
    "    # Fuzzy-match each level independently for spelling corrections\n",
    "    for raw_part in raw_parts:\n",
    "            if raw_part != \"\":\n",
    "                #print(\"Raw part:\", raw_part)\n",
    "                best, score = process.extractOne(raw_part, flat_no_duplicates)\n",
    "                if score >= threshold:\n",
    "                    reconstructed.append(best)\n",
    "                    #print(\"Normalizing category1:\", raw_part, \"->\", best)\n",
    "                else:\n",
    "                    reconstructed.append(raw_part) \n",
    "                    #print(\"Normalizing category2:\", raw_part) # keep raw if no good match\n",
    "    # Join reconstructed path\n",
    "    reconstructed_str = \" > \".join(reconstructed)\n",
    "    #print(\"Reconstructed category:\", reconstructed_str)\n",
    "\n",
    "    # Fuzzy match full reconstructed category to canonical list for order fixes\n",
    "    best_cat, best_score = process.extractOne(reconstructed_str, canonical_categories)\n",
    "\n",
    "    if best_score >= threshold:\n",
    "        return best_cat\n",
    "    return reconstructed_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff26066",
   "metadata": {},
   "source": [
    "#### Fix Category structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3d2513e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_file = Path(base_dir) / \"data\" / \"categories.json\"\n",
    "with open(category_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    canonical_categories = json.load(f)\n",
    "#print(canonical_categories)\n",
    "for product in products:\n",
    "    if \"category\" in product:\n",
    "       #print(product[\"category\"])\n",
    "       normalized_attrs = normalize_category(product[\"category\"], canonical_categories)\n",
    "       product[\"category\"] = normalized_attrs\n",
    "      # print(\"Normalized category:\", normalized_attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6fd3c1",
   "metadata": {},
   "source": [
    "##### Delete the _id in the products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d8b009af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vendor': 'Cordillera Steel', 'sku': 'W4E74DIEV', 'title': 'Industrial Lubricant industrial grade stainless steel', 'description': 'high-flow OEM abrasive bulk pack aftermarket industrial grade moulding bulk pack polycarbonate OEM tomato colored polycarbonate colour red aftermarket cables', 'unit_of_measure': 'gal', 'category': 'Electrical > Cables > Power Cords', 'attributes': {'hp': 3, 'pack_size': '12 pcs', 'diameter_mm': '50 mm'}, 'region_availability': ['CL'], 'supplier_rating': 2.8, 'inventory_status': 'out_of_stock', 'bulk_pack_size': '24 pcs'}\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "for product in products:\n",
    "    del product[\"_id\"]\n",
    "print(products[60])\n",
    "print(len(products))  # Print first 2 products to verify changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c223fdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed: 10000\n",
      "Failed to index: 0\n"
     ]
    }
   ],
   "source": [
    "#small_set = products[60]  # for testing\n",
    "es_bulk = es.options(request_timeout=7000)\n",
    "def gendata(products):\n",
    "    for p in products:\n",
    "        yield {\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": \"products\",\n",
    "            \"_source\": p\n",
    "        }\n",
    "try:\n",
    "    success, failed = helpers.bulk(\n",
    "        client=es_bulk,\n",
    "        actions=gendata(products),\n",
    "        chunk_size=100,\n",
    "        raise_on_error=False,\n",
    "        raise_on_exception=False\n",
    "    )\n",
    "\n",
    "    print(f\"Successfully indexed: {success}\")\n",
    "    print(f\"Failed to index: {len(failed)}\")\n",
    "\n",
    "    if failed:\n",
    "        print(\"\\n--- FAILED DOCUMENTS ---\")\n",
    "        for f in failed:\n",
    "            print(f)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Bulk ingest crashed: {e}\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ded381",
   "metadata": {},
   "source": [
    "#### Get the Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "70d1c425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'took': 1, 'timed_out': False, '_shards': {'total': 6, 'successful': 6, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 9735, 'relation': 'eq'}, 'max_score': None, 'hits': []}, 'aggregations': {'version': {'doc_count_error_upper_bound': 0, 'sum_other_doc_count': 0, 'buckets': [{'key': 'products', 'doc_count': 9344, 'NAME': {'doc_count_error_upper_bound': 0, 'sum_other_doc_count': 0, 'buckets': [{'key': 1, 'doc_count': 9079}, {'key': 2, 'doc_count': 265}]}}, {'key': 'products_errors', 'doc_count': 391, 'NAME': {'doc_count_error_upper_bound': 0, 'sum_other_doc_count': 0, 'buckets': [{'key': 1, 'doc_count': 391}]}}]}}}\n",
      "index: products\n",
      "    version 1: 9079\n",
      "    version 2: 265\n",
      "index: products_errors\n",
      "    version 1: 391\n"
     ]
    }
   ],
   "source": [
    "resp = es.search(\n",
    "   index=\"products,products_errors\",\n",
    "    size=\"0\",\n",
    "    aggs={\n",
    "        \"version\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"_index\"\n",
    "            },\n",
    "            \"aggs\": {\n",
    "                \"NAME\": {\n",
    "                    \"terms\": {\n",
    "                        \"field\": \"_version\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },)\n",
    "\n",
    "print(resp)\n",
    "for bucket in resp['aggregations']['version']['buckets']:\n",
    "    index_name = bucket['key']\n",
    "    print(f\"index: {index_name}\")\n",
    "    \n",
    "    for v in bucket['NAME']['buckets']:\n",
    "        version = v['key']\n",
    "        count = v['doc_count']\n",
    "        print(f\"    version {version}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a6f40c",
   "metadata": {},
   "source": [
    "#### Concludes the Ingestion Process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
